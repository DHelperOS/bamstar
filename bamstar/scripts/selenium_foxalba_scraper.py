#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Foxì•Œë°” Selenium ìŠ¤í¬ë˜í•‘ ìŠ¤í¬ë¦½íŠ¸
ë°¤ì•Œë°” ë§¤ì¹­ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±ìš©
"""

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
import json
import time
import random
from datetime import datetime
import csv

class SeleniumFoxAlbaScraper:
    def __init__(self):
        self.base_url = "https://www.foxalba.com"
        self.setup_driver()
    
    def setup_driver(self):
        """Chrome WebDriver ì„¤ì •"""
        chrome_options = Options()
        chrome_options.add_argument('--headless=new')  # ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        chrome_options.add_argument('--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        
        service = Service(ChromeDriverManager().install())
        self.driver = webdriver.Chrome(service=service, options=chrome_options)
        
        # WebDriver ê°ì§€ ë°©ì§€
        self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
    
    def scrape_job_list(self, max_pages=3):
        """êµ¬ì¸ì •ë³´ ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘"""
        all_jobs = []
        
        try:
            # ë©”ì¸ í˜ì´ì§€ë¡œ ì´ë™í•´ì„œ ì„¸ì…˜ ì‹œì‘
            print("ğŸŒ Foxì•Œë°” ë©”ì¸ í˜ì´ì§€ë¡œ ì´ë™ ì¤‘...")
            self.driver.get(self.base_url)
            time.sleep(3)
            
            # êµ¬ì¸ì •ë³´ í˜ì´ì§€ë¡œ ì´ë™
            job_list_url = "https://www.foxalba.com/offer/offer_list.asp"
            
            for page in range(1, max_pages + 1):
                print(f"ğŸ“„ í˜ì´ì§€ {page} ìŠ¤í¬ë˜í•‘ ì¤‘...")
                
                url = f"{job_list_url}?page={page}"
                self.driver.get(url)
                time.sleep(2)
                
                print(f"ğŸ“„ í˜„ì¬ í˜ì´ì§€ ì œëª©: {self.driver.title}")
                
                # í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸°
                try:
                    WebDriverWait(self.driver, 10).until(
                        lambda driver: driver.execute_script("return document.readyState") == "complete"
                    )
                except Exception as e:
                    print(f"âš ï¸ í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸° ì‹¤íŒ¨: {e}")
                
                # ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ êµ¬ì¸ì •ë³´ ìš”ì†Œë“¤ ì°¾ê¸°
                job_elements = self.find_job_elements()
                
                if job_elements:
                    print(f"ğŸ“‹ ë°œê²¬ëœ êµ¬ì¸ì •ë³´: {len(job_elements)}ê°œ")
                    
                    for element in job_elements:
                        job_data = self.extract_job_data_selenium(element)
                        if job_data:
                            all_jobs.append(job_data)
                            print(f"âœ… ìˆ˜ì§‘: {job_data.get('title', 'Unknown')}")
                else:
                    print(f"âŒ í˜ì´ì§€ {page}ì—ì„œ êµ¬ì¸ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    
                    # ë””ë²„ê¹…: í˜ì´ì§€ ì†ŒìŠ¤ ì¼ë¶€ ì¶œë ¥
                    page_source = self.driver.page_source[:1000]
                    print(f"ğŸ“ í˜ì´ì§€ ì†ŒìŠ¤ ìƒ˜í”Œ: {page_source}...")
                
                # í˜ì´ì§€ ê°„ ë”œë ˆì´
                time.sleep(random.uniform(2, 4))
            
            return all_jobs
            
        except Exception as e:
            print(f"âŒ ìŠ¤í¬ë˜í•‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return all_jobs
    
    def find_job_elements(self):
        """ë‹¤ì–‘í•œ ì…€ë ‰í„°ë¡œ êµ¬ì¸ì •ë³´ ìš”ì†Œë“¤ ì°¾ê¸°"""
        job_elements = []
        
        # ì‹œë„í•  ì…€ë ‰í„°ë“¤
        selectors = [
            "tr.bg_white, tr.bg_gray",
            ".job-item",
            ".offer-item", 
            "table tr:not(:first-child)",
            "tbody tr",
            ".list-item",
            "[class*='job']",
            "[class*='offer']"
        ]
        
        for selector in selectors:
            try:
                elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                if elements:
                    print(f"ğŸ¯ ì…€ë ‰í„° '{selector}'ë¡œ {len(elements)}ê°œ ìš”ì†Œ ë°œê²¬")
                    job_elements = elements
                    break
            except Exception as e:
                print(f"âš ï¸ ì…€ë ‰í„° '{selector}' ì‹¤íŒ¨: {e}")
                continue
        
        return job_elements
    
    def extract_job_data_selenium(self, element):
        """Selenium ìš”ì†Œì—ì„œ ë°ì´í„° ì¶”ì¶œ"""
        try:
            data = {}
            
            # ìš”ì†Œ ë‚´ ëª¨ë“  í…ìŠ¤íŠ¸ ì¶”ì¶œ
            text_content = element.text.strip()
            if not text_content:
                return None
            
            # td ìš”ì†Œë“¤ ì°¾ê¸°
            tds = element.find_elements(By.TAG_NAME, "td")
            
            if len(tds) >= 3:
                # í…Œì´ë¸” êµ¬ì¡°ë¡œ íŒŒì‹±
                data['title'] = tds[0].text.strip() if len(tds) > 0 else ''
                data['location'] = tds[1].text.strip() if len(tds) > 1 else ''
                data['salary'] = tds[2].text.strip() if len(tds) > 2 else ''
                data['time'] = tds[3].text.strip() if len(tds) > 3 else ''
                data['date'] = tds[4].text.strip() if len(tds) > 4 else ''
            else:
                # ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì •ë³´ ì¶”ì¶œ
                lines = text_content.split('\n')
                data['title'] = lines[0] if len(lines) > 0 else text_content
                data['raw_text'] = text_content
            
            # ë§í¬ ì°¾ê¸°
            try:
                link_element = element.find_element(By.TAG_NAME, "a")
                data['url'] = link_element.get_attribute("href")
            except:
                pass
            
            # í•„ìˆ˜ ë°ì´í„° ê²€ì¦
            if data.get('title') or data.get('raw_text'):
                return data
                
        except Exception as e:
            print(f"âš ï¸ ë°ì´í„° ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        
        return None
    
    def convert_to_nightjob_data(self, jobs_data):
        """ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ë°¤ì•Œë°” ë§¤ì¹­ ë°ì´í„°ë¡œ ë³€í™˜"""
        matching_data = []
        
        nightjob_types = [
            'ë£¸', 'í¼ë¸”ë¦­', 'ê°•ë‚¨í´ëŸ½', 'í™ëŒ€í´ëŸ½', 'KTV', 'ë…¸ë˜ë°©', 
            'ë°”', 'í˜¸ìŠ¤íŠ¸ë°”', 'ì£¼ì ', 'ì¹´í˜', 'ë¼ìš´ì§€', 'í•˜ì´í¼ë¸”ë¦­',
            'í…ì¹´í˜', 'ì…”ì¸ ë£¸', 'ë¹„ì¦ˆë‹ˆìŠ¤í´ëŸ½', 'ë§ˆì‚¬ì§€', 'ì•ˆë§ˆ',
            'ìœ í¥ì£¼ì ', 'ë‹¨ë€ì£¼ì ', 'ë‚˜ì´íŠ¸í´ëŸ½'
        ]
        
        areas = [
            'ê°•ë‚¨', 'í™ëŒ€', 'ì´íƒœì›', 'ê±´ëŒ€', 'ì‹ ì´Œ', 'ëª…ë™', 'ì••êµ¬ì •', 
            'ì²­ë‹´', 'ë…¼í˜„', 'ì—­ì‚¼', 'ì„ ë¦‰', 'ì‚¼ì„±', 'ë°©ë°°', 'ì„œì´ˆ',
            'ì¢…ë¡œ', 'ì„ì§€ë¡œ', 'ë™ëŒ€ë¬¸', 'ì ì‹¤', 'ì†¡íŒŒ', 'ê°•ë™',
            'ë§ˆí¬', 'ìš©ì‚°', 'ì„±ë¶', 'ê°•ë¶'
        ]
        
        for idx, job in enumerate(jobs_data):
            place_profile = {
                'user_id': f'nightjob_place_{idx + 1}',
                'place_name': self.generate_place_name(nightjob_types),
                'business_type': random.choice(nightjob_types),
                'address': f'ì„œìš¸ì‹œ {random.choice(areas)}êµ¬',
                'latitude': None,
                'longitude': None,
                'manager_gender': random.choice(['ë‚¨', 'ì—¬']),
                'offered_min_pay': random.randint(150000, 300000),  # ë°¤ì•Œë°” ê¸‰ì—¬ ìˆ˜ì¤€
                'offered_max_pay': random.randint(300000, 800000),
                'desired_experience_level': random.choice(['NEWCOMER', 'JUNIOR', 'INTERMEDIATE']),
                'profile_image_urls': [],
                'work_time': random.choice(['ì˜¤í›„ 6ì‹œ~ì˜¤ì „ 2ì‹œ', 'ì˜¤í›„ 8ì‹œ~ì˜¤ì „ 4ì‹œ', 'ì˜¤í›„ 9ì‹œ~ì˜¤ì „ 3ì‹œ']),
                'job_description': job.get('title', job.get('raw_text', ''))[:100],
                'posted_date': datetime.now().strftime('%Y-%m-%d'),
                'original_data': job
            }
            
            matching_data.append(place_profile)
        
        return matching_data
    
    def generate_place_name(self, business_types):
        """ì—…ì¢…ë³„ ì ì ˆí•œ ì—…ì²´ëª… ìƒì„±"""
        prefixes = ['í”„ë¦¬ë¯¸ì—„', 'ê³ ê¸‰', 'ë¡œì–„', 'ë¹…', 'íƒ‘', 'í”„ë¼ì„', 'VIP', 'ê³¨ë“œ', 'í”Œë˜í‹°ë„˜']
        suffixes = ['í´ëŸ½', 'ë¼ìš´ì§€', 'ë°”', 'ë£¸', 'KTV', 'í¼ë¸”ë¦­', 'ì¹´í˜']
        areas = ['ê°•ë‚¨', 'í™ëŒ€', 'ì••êµ¬ì •', 'ì²­ë‹´', 'ì´íƒœì›', 'ê±´ëŒ€', 'ì‹ ì´Œ']
        
        name_patterns = [
            f"{random.choice(prefixes)}{random.choice(areas)}{random.choice(suffixes)}",
            f"{random.choice(areas)}{random.choice(prefixes)}{random.choice(suffixes)}",
            f"{random.choice(['í´ëŸ½', 'ë°”', 'ë£¸'])}{random.choice(['A', 'B', 'M', 'K', 'J'])}"
        ]
        
        return random.choice(name_patterns)
    
    def save_data(self, data, filename_prefix="nightjob_data"):
        """ë°ì´í„° ì €ì¥"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # JSON ì €ì¥
        json_filename = f"{filename_prefix}_{timestamp}.json"
        with open(json_filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“„ JSON íŒŒì¼ ì €ì¥: {json_filename}")
        
        # CSV ì €ì¥
        if data:
            csv_filename = f"{filename_prefix}_{timestamp}.csv"
            with open(csv_filename, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=data[0].keys())
                writer.writeheader()
                writer.writerows(data)
            print(f"ğŸ“Š CSV íŒŒì¼ ì €ì¥: {csv_filename}")
        
        return json_filename, csv_filename
    
    def close(self):
        """ë“œë¼ì´ë²„ ì¢…ë£Œ"""
        if self.driver:
            self.driver.quit()

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸŒ™ ë°¤ì•Œë°” ë§¤ì¹­ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘!")
    
    scraper = SeleniumFoxAlbaScraper()
    
    try:
        # 1. Foxì•Œë°”ì—ì„œ ë°ì´í„° ìˆ˜ì§‘
        print("\nğŸ“‹ Foxì•Œë°” êµ¬ì¸ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
        jobs_data = scraper.scrape_job_list(max_pages=3)
        
        if not jobs_data:
            print("âŒ ì‹¤ì œ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨. ê°€ìƒ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
            # ê°€ìƒ ë°ì´í„° ìƒì„±
            jobs_data = [
                {'title': f'ë°¤ì•Œë°” êµ¬ì¸ {i}', 'raw_text': f'êµ¬ì¸ì •ë³´ {i}'} 
                for i in range(1, 21)
            ]
        
        print(f"\nâœ… ì´ {len(jobs_data)}ê°œì˜ êµ¬ì¸ì •ë³´ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
        
        # 2. ì›ë³¸ ë°ì´í„° ì €ì¥
        print("\nğŸ’¾ ì›ë³¸ ë°ì´í„° ì €ì¥ ì¤‘...")
        scraper.save_data(jobs_data, "foxalba_raw")
        
        # 3. ë°¤ì•Œë°” ë§¤ì¹­ ë°ì´í„°ë¡œ ë³€í™˜
        print("\nğŸ”„ ë°¤ì•Œë°” ë§¤ì¹­ ë°ì´í„°ë¡œ ë³€í™˜ ì¤‘...")
        matching_data = scraper.convert_to_nightjob_data(jobs_data)
        
        # 4. ë§¤ì¹­ ë°ì´í„° ì €ì¥
        print("\nğŸ’¾ ë§¤ì¹­ ë°ì´í„° ì €ì¥ ì¤‘...")
        scraper.save_data(matching_data, "nightjob_matching")
        
        # 5. ìƒ˜í”Œ ë°ì´í„° ì¶œë ¥
        print("\nğŸŒ™ ìƒì„±ëœ ë°¤ì•Œë°” ë§¤ì¹­ ë°ì´í„° ìƒ˜í”Œ:")
        for i, job in enumerate(matching_data[:5]):
            print(f"\n{i+1}. {job.get('place_name', 'Unknown')}")
            print(f"   ì—…ì¢…: {job.get('business_type', 'Unknown')}")
            print(f"   ì§€ì—­: {job.get('address', 'Unknown')}")
            print(f"   ê¸‰ì—¬: {job.get('offered_min_pay', 0):,}ì› ~ {job.get('offered_max_pay', 0):,}ì›")
            print(f"   ì‹œê°„: {job.get('work_time', 'Unknown')}")
        
        print(f"\nğŸ‰ ë°ì´í„° ìƒì„± ì™„ë£Œ! ì´ {len(matching_data)}ê°œì˜ ë°¤ì•Œë°” ë§¤ì¹­ ë°ì´í„°ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")
        
    finally:
        scraper.close()

if __name__ == "__main__":
    main()