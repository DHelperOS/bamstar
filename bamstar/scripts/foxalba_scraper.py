#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Foxì•Œë°” êµ¬ì¸ì •ë³´ ìŠ¤í¬ë˜í•‘ ìŠ¤í¬ë¦½íŠ¸
ë§¤ì¹­ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±ìš©
"""

import requests
from bs4 import BeautifulSoup
import json
import time
import random
from urllib.parse import urljoin, urlparse
import csv
from datetime import datetime

class FoxAlbaScraper:
    def __init__(self):
        self.base_url = "https://www.foxalba.com"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'max-age=0'
        })
        
        # ì²« ë°©ë¬¸ìœ¼ë¡œ ì„¸ì…˜ ì¿ í‚¤ ë°›ê¸°
        try:
            self.session.get(self.base_url)
        except:
            pass
        
    def scrape_job_list(self, max_pages=3):
        """êµ¬ì¸ì •ë³´ ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘"""
        all_jobs = []
        
        for page in range(1, max_pages + 1):
            print(f"ğŸ“„ í˜ì´ì§€ {page} ìŠ¤í¬ë˜í•‘ ì¤‘...")
            
            url = f"https://www.foxalba.com/offer/offer_list.asp?page={page}"
            
            try:
                response = self.session.get(url)
                response.raise_for_status()
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # HTML êµ¬ì¡° ë””ë²„ê¹…
                print(f"ğŸ” í˜ì´ì§€ {page} HTML êµ¬ì¡° í™•ì¸...")
                print(f"ğŸ“„ Title: {soup.title.string if soup.title else 'No title'}")
                
                # ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ êµ¬ì¸ì •ë³´ í•­ëª©ë“¤ ì°¾ê¸° ì‹œë„
                job_items = []
                
                # ë°©ë²• 1: table row ì°¾ê¸°
                job_items = soup.find_all('tr', class_=['bg_white', 'bg_gray'])
                
                # ë°©ë²• 2: div ê¸°ë°˜ìœ¼ë¡œ ì°¾ê¸°
                if not job_items:
                    job_items = soup.find_all('div', class_=['job-item', 'job_list', 'offer-item'])
                
                # ë°©ë²• 3: ì¼ë°˜ì ì¸ í…Œì´ë¸” êµ¬ì¡° ì°¾ê¸°
                if not job_items:
                    tables = soup.find_all('table')
                    for table in tables:
                        rows = table.find_all('tr')
                        if len(rows) > 1:  # í—¤ë” ì œì™¸í•˜ê³  ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
                            job_items = rows[1:]  # ì²« ë²ˆì§¸ëŠ” í—¤ë”ì´ë¯€ë¡œ ì œì™¸
                            break
                
                # ë°©ë²• 4: ëª¨ë“  tr íƒœê·¸ ì°¾ê¸°
                if not job_items:
                    job_items = soup.find_all('tr')[1:] if soup.find_all('tr') else []
                
                print(f"ğŸ“‹ ë°œê²¬ëœ í•­ëª© ìˆ˜: {len(job_items)}")
                
                if not job_items:
                    # ì‘ë‹µ ë‚´ìš© ì¼ë¶€ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
                    print(f"ğŸ“ ì‘ë‹µ ë‚´ìš© ìƒ˜í”Œ: {str(soup)[:500]}...")
                    continue
                
                for item in job_items:
                    job_data = self.extract_job_data(item)
                    if job_data:
                        all_jobs.append(job_data)
                        print(f"âœ… ìˆ˜ì§‘: {job_data.get('company_name', 'Unknown')} - {job_data.get('job_title', 'Unknown')}")
                
                # ìš”ì²­ ê°„ ë”œë ˆì´ (ì„œë²„ ë¶€í•˜ ë°©ì§€)
                time.sleep(random.uniform(1, 2))
                
            except requests.RequestException as e:
                print(f"âŒ í˜ì´ì§€ {page} ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
                continue
        
        return all_jobs
    
    def extract_job_data(self, item):
        """ê°œë³„ êµ¬ì¸ì •ë³´ ë°ì´í„° ì¶”ì¶œ"""
        try:
            data = {}
            
            # ëª¨ë“  td ìš”ì†Œë“¤ì„ ê°€ì ¸ì™€ì„œ ìˆœì„œëŒ€ë¡œ íŒŒì‹±
            tds = item.find_all('td')
            
            if len(tds) >= 4:
                # ì¼ë°˜ì ì¸ êµ¬ì¸ì •ë³´ í…Œì´ë¸” êµ¬ì¡° ì¶”ì •
                data['company_name'] = tds[0].get_text(strip=True) if len(tds) > 0 else ''
                data['job_title'] = tds[1].get_text(strip=True) if len(tds) > 1 else ''
                data['location'] = tds[2].get_text(strip=True) if len(tds) > 2 else ''
                data['salary'] = tds[3].get_text(strip=True) if len(tds) > 3 else ''
                data['work_time'] = tds[4].get_text(strip=True) if len(tds) > 4 else ''
                data['posted_date'] = tds[5].get_text(strip=True) if len(tds) > 5 else ''
                
                # ìƒì„¸ ë§í¬ ì°¾ê¸°
                link_elem = item.find('a')
                if link_elem and link_elem.get('href'):
                    data['detail_url'] = urljoin(self.base_url, link_elem['href'])
            else:
                # class ê¸°ë°˜ìœ¼ë¡œ ì‹œë„
                # íšŒì‚¬ëª…/ì—…ì²´ëª…
                company_elem = item.find('td', class_='company') or item.find(text=lambda t: t and 'íšŒì‚¬' in str(t))
                if company_elem:
                    data['company_name'] = company_elem.get_text(strip=True) if hasattr(company_elem, 'get_text') else str(company_elem).strip()
                
                # ëª¨ë“  í…ìŠ¤íŠ¸ ì¶”ì¶œí•´ì„œ ë¶„ì„
                all_text = item.get_text(strip=True)
                if all_text:
                    # í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ì„ì‹œë¡œ ì €ì¥
                    data['raw_text'] = all_text
                    data['company_name'] = data.get('company_name', 'Unknown')
                    data['job_title'] = all_text[:50] + '...' if len(all_text) > 50 else all_text
            
            # ê¸°ë³¸ ê²€ì¦: ìµœì†Œí•œ í…ìŠ¤íŠ¸ê°€ ìˆëŠ”ì§€ í™•ì¸
            if data.get('company_name') or data.get('job_title') or data.get('raw_text'):
                return data
            
        except Exception as e:
            print(f"âš ï¸ ë°ì´í„° ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        
        return None
    
    def convert_to_matching_data(self, jobs_data):
        """Foxì•Œë°” ë°ì´í„°ë¥¼ ë§¤ì¹­ ì„œë¹„ìŠ¤ìš© ë°ì´í„°ë¡œ ë³€í™˜"""
        matching_data = []
        
        for idx, job in enumerate(jobs_data):
            # Place Profile ë°ì´í„°ë¡œ ë³€í™˜
            place_profile = {
                'user_id': f'foxalba_place_{idx + 1}',
                'place_name': job.get('company_name', ''),
                'business_type': self.extract_business_type(job.get('job_title', '')),
                'address': job.get('location', ''),
                'latitude': None,  # ì‹¤ì œ ì„œë¹„ìŠ¤ì—ì„œëŠ” ì£¼ì†Œë¥¼ ì¢Œí‘œë¡œ ë³€í™˜ í•„ìš”
                'longitude': None,
                'manager_gender': random.choice(['ë‚¨', 'ì—¬']),  # ì„ì˜ ìƒì„±
                'offered_min_pay': self.extract_min_salary(job.get('salary', '')),
                'offered_max_pay': self.extract_max_salary(job.get('salary', '')),
                'desired_experience_level': random.choice(['NEWCOMER', 'JUNIOR', 'INTERMEDIATE']),
                'profile_image_urls': [],  # ì‹¤ì œ ì´ë¯¸ì§€ URL í•„ìš”ì‹œ ì¶”ê°€
                'work_time': job.get('work_time', ''),
                'job_description': job.get('job_title', ''),
                'posted_date': job.get('posted_date', ''),
                'detail_url': job.get('detail_url', '')
            }
            
            matching_data.append(place_profile)
        
        return matching_data
    
    def extract_business_type(self, job_title):
        """ì§ë¬´ ì œëª©ì—ì„œ ì—…ì¢… ì¶”ì¶œ"""
        business_types = {
            'ì¹´í˜': ['ì¹´í˜', 'ì»¤í”¼', 'ìŠ¤íƒ€ë²…ìŠ¤', 'ì´ë””ì•¼', 'í• ë¦¬ìŠ¤'],
            'ìŒì‹ì ': ['ìŒì‹ì ', 'ì‹ë‹¹', 'ìš”ë¦¬', 'ì£¼ë°©', 'ì„œë¹™', 'í™€', 'ì¹˜í‚¨', 'í”¼ì', 'í•œì‹', 'ì¤‘ì‹', 'ì¼ì‹', 'ì–‘ì‹'],
            'í¸ì˜ì ': ['í¸ì˜ì ', 'GS25', 'CU', 'ì„¸ë¸ì¼ë ˆë¸', 'ë¯¸ë‹ˆìŠ¤í†±'],
            'ë§ˆíŠ¸': ['ë§ˆíŠ¸', 'ì´ë§ˆíŠ¸', 'ë¡¯ë°ë§ˆíŠ¸', 'í™ˆí”ŒëŸ¬ìŠ¤', 'ì½”ìŠ¤íŠ¸ì½”'],
            'í•™ì›': ['í•™ì›', 'êµìŠµ', 'ê³¼ì™¸', 'ê°•ì‚¬', 'ì„ ìƒ'],
            'ì‚¬ë¬´ì§': ['ì‚¬ë¬´', 'ê´€ë¦¬', 'íšŒê³„', 'ê²½ë¦¬', 'ì´ë¬´'],
            'íŒë§¤ì§': ['íŒë§¤', 'ì˜ì—…', 'ê³ ê°ìƒë‹´', 'í…”ë ˆë§ˆì¼€íŒ…'],
            'ì„œë¹„ìŠ¤ì§': ['ì²­ì†Œ', 'ê²½ë¹„', 'ë°°ë‹¬', 'ìš´ì „', 'íƒë°°'],
        }
        
        for business_type, keywords in business_types.items():
            if any(keyword in job_title for keyword in keywords):
                return business_type
        
        return 'ê¸°íƒ€'
    
    def extract_min_salary(self, salary_text):
        """ê¸‰ì—¬ í…ìŠ¤íŠ¸ì—ì„œ ìµœì†Œ ê¸‰ì—¬ ì¶”ì¶œ"""
        try:
            # ìˆ«ìë§Œ ì¶”ì¶œ
            import re
            numbers = re.findall(r'\d+', salary_text.replace(',', ''))
            if numbers:
                return int(numbers[0])
        except:
            pass
        return 10000  # ê¸°ë³¸ê°’
    
    def extract_max_salary(self, salary_text):
        """ê¸‰ì—¬ í…ìŠ¤íŠ¸ì—ì„œ ìµœëŒ€ ê¸‰ì—¬ ì¶”ì¶œ"""
        try:
            import re
            numbers = re.findall(r'\d+', salary_text.replace(',', ''))
            if len(numbers) >= 2:
                return int(numbers[-1])
            elif len(numbers) == 1:
                return int(numbers[0]) + 2000  # ìµœì†Œê¸‰ì—¬ + 2000ì›
        except:
            pass
        return 12000  # ê¸°ë³¸ê°’
    
    def save_data(self, data, filename_prefix="foxalba_jobs"):
        """ë°ì´í„°ë¥¼ JSONê³¼ CSV íŒŒì¼ë¡œ ì €ì¥"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # JSON ì €ì¥
        json_filename = f"{filename_prefix}_{timestamp}.json"
        with open(json_filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“„ JSON íŒŒì¼ ì €ì¥: {json_filename}")
        
        # CSV ì €ì¥
        if data:
            csv_filename = f"{filename_prefix}_{timestamp}.csv"
            with open(csv_filename, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=data[0].keys())
                writer.writeheader()
                writer.writerows(data)
            print(f"ğŸ“Š CSV íŒŒì¼ ì €ì¥: {csv_filename}")
        
        return json_filename, csv_filename

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ¦Š Foxì•Œë°” êµ¬ì¸ì •ë³´ ìŠ¤í¬ë˜í•‘ ì‹œì‘!")
    
    scraper = FoxAlbaScraper()
    
    # 1. êµ¬ì¸ì •ë³´ ë¦¬ìŠ¤íŠ¸ ìŠ¤í¬ë˜í•‘ (3í˜ì´ì§€)
    print("\nğŸ“‹ êµ¬ì¸ì •ë³´ ë¦¬ìŠ¤íŠ¸ ìŠ¤í¬ë˜í•‘ ì¤‘...")
    jobs_data = scraper.scrape_job_list(max_pages=3)
    
    if not jobs_data:
        print("âŒ ìŠ¤í¬ë˜í•‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"\nâœ… ì´ {len(jobs_data)}ê°œì˜ êµ¬ì¸ì •ë³´ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
    
    # 2. ì›ë³¸ ë°ì´í„° ì €ì¥
    print("\nğŸ’¾ ì›ë³¸ ë°ì´í„° ì €ì¥ ì¤‘...")
    scraper.save_data(jobs_data, "foxalba_raw")
    
    # 3. ë§¤ì¹­ ì„œë¹„ìŠ¤ìš© ë°ì´í„°ë¡œ ë³€í™˜
    print("\nğŸ”„ ë§¤ì¹­ ì„œë¹„ìŠ¤ìš© ë°ì´í„°ë¡œ ë³€í™˜ ì¤‘...")
    matching_data = scraper.convert_to_matching_data(jobs_data)
    
    # 4. ë³€í™˜ëœ ë°ì´í„° ì €ì¥
    print("\nğŸ’¾ ë§¤ì¹­ ë°ì´í„° ì €ì¥ ì¤‘...")
    scraper.save_data(matching_data, "foxalba_matching")
    
    # 5. ìƒ˜í”Œ ë°ì´í„° ì¶œë ¥
    print("\nğŸ“„ ìˆ˜ì§‘ëœ ë°ì´í„° ìƒ˜í”Œ:")
    for i, job in enumerate(jobs_data[:3]):
        print(f"\n{i+1}. {job.get('company_name', 'Unknown')}")
        print(f"   ì œëª©: {job.get('job_title', 'Unknown')}")
        print(f"   ì§€ì—­: {job.get('location', 'Unknown')}")
        print(f"   ê¸‰ì—¬: {job.get('salary', 'Unknown')}")
    
    print(f"\nğŸ‰ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ! ì´ {len(jobs_data)}ê°œì˜ êµ¬ì¸ì •ë³´ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
    print("ğŸ“ ìƒì„±ëœ íŒŒì¼ë“¤ì„ í™•ì¸í•´ë³´ì„¸ìš”.")

if __name__ == "__main__":
    main()